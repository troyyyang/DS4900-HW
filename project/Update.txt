Troy Yang
4/9/2019
Project Update
        Please note in addition to this report I have also included a notebook with the full evaluation metrics.
        For this week, I set out with the overarching goal that we discussed to see if I could train a model that performs better than the pre-trained model I found this week. In addition to that, we had also discussed some other things in last week's meeting, such as text preprocessing, and trying an ensemble model that uses boosting to focus on data points that were incorrectly classified. 
        Before I could begin with training a model, I first had to go find a more robust dataset that I could train and evaluate models on. Because the source of my data is Twitter, I knew I wanted to get labeled data that also came from Twitter. I initially was going to use a dataset I found on Kaggle that contained 100k labelled tweets, however after doing some initial work with the data I realized I wanted a dataset that contained positive, negative and neutral labels, instead of just positive and negative. Without digging too much I was able to find a suitable dataset on Kaggle, provided by the machine learning company called Figure Eight. This dataset contains just under 15,000 tweets from February of 2015. These tweets were made by airline customers describing their experiences with different airlines. Although I did have 100 tweets that I labelled myself, that did not seem like nearly robust enough dataset to build a model that would not suffer from tremendous overfitting. Although the nouns the the tweets are referring to might be different (airlines vs hockey), this did not concern me at all because the objective of the model is to classify sentiment, not classify if tweets are about sports vs airline companies vs anything else. The dataset can be found at this link: https://www.kaggle.com/crowdflower/twitter-airline-sentiment#Tweets.csv .
        Once I had my dataset, I could then begin working on figuring out how to address some of the things we had discussed. With regards to pre-processing, I wrote a helper function that removes usernames, hashtags, URLs, excess punctuation as well as excess whitespace. When doing background research into Natural Language Processing, I also found that lemmatization is an important step when it comes to pre-processing. In case you are not familiar, long story short lemmatization is very similar to stemming, in that if a word is not in its most “raw” or “reduced” form, applying lemmatization would “reduce” it. For example lemmatization would reduce runs to run. 
        After briefly evaluating some other more NLP specific libraries, it seemed fairly obvious that sklearn should be the main library I use to try to create a better model. A couple features that made sklearn the clear choice for me were the pipelines, parameter search grids, and robust evaluation metrics. The three models I knew I wanted to try were logistic regression, naive bayes, and support vector machines because these were models I had a decent understanding of because I had to implement them in my machine learning class, but also because other people have gotten decent performance for NLP tasks with said algorithms. Based on discussions we had last week, I decided that I would also try Adaboost. I also implemented Adaboost in my machine learning class, but in addition to that it is an ensemble model that at each iteration trains a new classifier giving more weight to the points the previous classifier got wrong. 
        I did run into an issue with the support vector machines. For some reason when training SVMs, my laptop would run for a little bit and then all of a sudden shut off. I tried playing with the parameters I was using but I was still faced with the same issue. A half dozen or so shutdowns later I decided to give up and not use SVMs.
        When I finally got the results for the other classifiers, I was honestly surprised at how bad the pre-trained model performed, especially compared to the models I trained. I trained my models and found the best parameters on a subset of the airline tweets using 3 folds cross validation. Once I had the best parameters for the given algorithm I then evaluated the tuned models on the testing data, along with the pre-trained Vader model on the testing data. I use the F1 scores to evaluate the models, as it is the most robust single metric (as far as I know).The pre-trained Vader model was by far the worst, with Naive Bayes, Logistic Regression and Adaboost all performing much better. Of the three models I tuned, Logistic Regression performed the best on test data. I found that all four models had similar fluctuations in performance based on the class. Every model did worst on neutral, better on positive, and best on negative. I was consider wrapping the classifiers with some voting to create an ensemble method, but this would’ve only made sense if it were clear that models were better at predicting some classes than others.
        Now that I have my final classifier, I am going to work on the acquiring the collection of tweets I want to classify, classifying the tweets and then running some analysis, and then evaluating my model with 2 other people and then writing the report. Please let me know if you think I should change anything in my next steps.